{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8983bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: we had RETIGHTEN ALL P CLIPS, NUTS, AND BOLTS AS NECESSARYALSO FOUND TWO BULKHEAD CONNECTORS, NOT FULLY LOCKED LEFT REAR OF MACHINE AND ONE ON RIGHT BOOMRUNNING TESTS NO FAULT\n",
      "Corrected: We had TIGHTEN ALL P CLIPS, NUTS, AND BOLTS AS NECESSARIANS FOUND TWO BULKHEAD CONNECTORS, NOT FULLY LOCKED LEFT REAR OF MACHINE AND ONE ON RIGHT OUTRUNNING TESTS NO FAULT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Taxonomy Sheet\n",
    "taxonomy_df = pd.read_excel('Task1.xlsx', sheet_name='Taxonomy')\n",
    "\n",
    "# Extract and clean taxonomy lists\n",
    "root_causes = taxonomy_df['Root Cause'].dropna().tolist()\n",
    "symptom_conditions = taxonomy_df['Symptom Condition '].dropna().tolist()\n",
    "symptom_components = taxonomy_df['Symptom Component'].dropna().tolist()\n",
    "fix_conditions = taxonomy_df['Fix Condition'].dropna().tolist()\n",
    "fix_components = taxonomy_df['Fix Component'].dropna().tolist()\n",
    "\n",
    "def get_lemmas(text):\n",
    "    \"\"\"Convert text into a list of lemmatized tokens.\"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def lemmatize_term(term):\n",
    "    \"\"\"Lemmatize a given phrase.\"\"\"\n",
    "    term = term.replace(\"-\", \" \")\n",
    "    doc = nlp(term.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Preprocess for better matching\n",
    "root_causes_lem = [lemmatize_term(rc) for rc in root_causes]\n",
    "symptom_conditions_lem = [lemmatize_term(sc) for sc in symptom_conditions]\n",
    "symptom_components_lem = [lemmatize_term(scm) for scm in symptom_components]\n",
    "fix_conditions_lem = [lemmatize_term(fc) for fc in fix_conditions]\n",
    "fix_components_lem = [lemmatize_term(fcomp) for fcomp in fix_components]\n",
    "\n",
    "\n",
    "def is_match(term_lem, combined_lemmas, combined_text):\n",
    "    \"\"\"Check for match using exact lemmatized token or fuzzy matching.\"\"\"\n",
    "    if ' ' not in term_lem:\n",
    "        return term_lem in combined_lemmas\n",
    "    else:\n",
    "        return fuzz.token_set_ratio(term_lem, combined_text) > 70\n",
    "\n",
    "def extract_matches(row, targets_lem, labels, max_items=3):\n",
    "    \"\"\"Extract up to max_items matches from the text.\"\"\"\n",
    "    text = row[\"Complaint\"] + \" \" + row[\"Cause\"] + \" \" + row[\"Correction\"]\n",
    "    combined_lemmas = get_lemmas(text)\n",
    "    combined_text = \" \".join(combined_lemmas)\n",
    "    found = []\n",
    "    covered_tokens = set()\n",
    "\n",
    "    for idx, target_lem in enumerate(targets_lem):\n",
    "        if is_match(target_lem, combined_lemmas, combined_text):\n",
    "            target_tokens = set(target_lem.split())\n",
    "            if not target_tokens.issubset(covered_tokens):\n",
    "                found.append(labels[idx])\n",
    "                covered_tokens.update(target_tokens)\n",
    "                if len(found) == max_items:\n",
    "                    break\n",
    "\n",
    "    found += [None] * (max_items - len(found))\n",
    "    return pd.Series(found)\n",
    "\n",
    "def detect_root_cause(row):\n",
    "    \"\"\"Detect root cause from the 'Cause' field.\"\"\"\n",
    "    text = row[\"Cause\"]\n",
    "    combined_lemmas = get_lemmas(text)\n",
    "    combined_text = \" \".join(combined_lemmas)\n",
    "    for idx, rc_lem in enumerate(root_causes_lem):\n",
    "        if is_match(rc_lem, combined_lemmas, combined_text):\n",
    "            return root_causes[idx]\n",
    "    return \"Not Mentioned\"\n",
    "\n",
    "def extract_fix_components_prioritized(row, max_items=3):\n",
    "    \"\"\"Extract fix components giving priority to those matching symptom components.\"\"\"\n",
    "    text = row[\"Complaint\"] + \" \" + row[\"Cause\"] + \" \" + row[\"Correction\"]\n",
    "    combined_lemmas = get_lemmas(text)\n",
    "    combined_text = \" \".join(combined_lemmas)\n",
    "\n",
    "    result = [None] * max_items\n",
    "    used_components = set()\n",
    "\n",
    "    # Priority matching using symptom components\n",
    "    for i in range(max_items):\n",
    "        symptom_component = row.get(f\"Symptom Component {i+1}\")\n",
    "        if symptom_component and symptom_component in fix_components:\n",
    "            result[i] = symptom_component\n",
    "            used_components.add(symptom_component)\n",
    "\n",
    "    # fuzzy match from the full text\n",
    "    matches = []\n",
    "    for idx, target_lem in enumerate(fix_components_lem):\n",
    "        label = fix_components[idx]\n",
    "        if label in used_components:\n",
    "            continue\n",
    "        if is_match(target_lem, combined_lemmas, combined_text):\n",
    "            matches.append(label)\n",
    "\n",
    "    # Fill remaining positions\n",
    "    j = 0\n",
    "    for i in range(max_items):\n",
    "        if result[i] is None and j < len(matches):\n",
    "            result[i] = matches[j]\n",
    "            used_components.add(matches[j])\n",
    "            j += 1\n",
    "\n",
    "    return pd.Series(result)\n",
    "\n",
    "def align_conditions(df, condition_prefix, component_prefix):\n",
    "    \"\"\"Align condition values with components if any condition field is missing.\"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        for i in range(2, 4):\n",
    "            cond_col = f\"{condition_prefix} {i}\"\n",
    "            comp_col = f\"{component_prefix} {i}\"\n",
    "            if pd.notna(row[comp_col]) and pd.isna(row[cond_col]):\n",
    "                for j in range(i - 1, 0, -1):\n",
    "                    prev_cond_col = f\"{condition_prefix} {j}\"\n",
    "                    if pd.notna(row[prev_cond_col]):\n",
    "                        df.at[idx, cond_col] = row[prev_cond_col]\n",
    "                        break\n",
    "    return df\n",
    "\n",
    "# Load Task data\n",
    "task_df = pd.read_excel('Task1.xlsx', sheet_name='Task')\n",
    "task_df[\"Order Date\"] = task_df[\"Order Date\"].dt.date\n",
    "\n",
    "# Tagging\n",
    "task_df[\"Root Cause\"] = task_df.apply(detect_root_cause, axis=1)\n",
    "\n",
    "task_df[[\"Symptom Condition 1\", \"Symptom Condition 2\", \"Symptom Condition 3\"]] = task_df.apply(\n",
    "    lambda row: extract_matches(row, symptom_conditions_lem, symptom_conditions), axis=1)\n",
    "\n",
    "task_df[[\"Symptom Component 1\", \"Symptom Component 2\", \"Symptom Component 3\"]] = task_df.apply(\n",
    "    lambda row: extract_matches(row, symptom_components_lem, symptom_components), axis=1)\n",
    "\n",
    "task_df[[\"Fix Condition 1\", \"Fix Condition 2\", \"Fix Condition 3\"]] = task_df.apply(\n",
    "    lambda row: extract_matches(row, fix_conditions_lem, fix_conditions), axis=1)\n",
    "\n",
    "task_df[[\"Fix Component 1\", \"Fix Component 2\", \"Fix Component 3\"]] = task_df.apply(\n",
    "    extract_fix_components_prioritized, axis=1)\n",
    "\n",
    "# Align Related Conditions\n",
    "task_df = align_conditions(task_df, \"Symptom Condition\", \"Symptom Component\")\n",
    "task_df = align_conditions(task_df, \"Fix Condition\", \"Fix Component\")\n",
    "\n",
    "output_columns = [\n",
    "    \"Primary Key\", \"Order Date\", \"Product Category\", \"Complaint\", \"Cause\", \"Correction\",\n",
    "    \"Root Cause\",\n",
    "    \"Symptom Condition 1\", \"Symptom Component 1\",\n",
    "    \"Symptom Condition 2\", \"Symptom Component 2\",\n",
    "    \"Symptom Condition 3\", \"Symptom Component 3\",\n",
    "    \"Fix Condition 1\", \"Fix Component 1\",\n",
    "    \"Fix Condition 2\", \"Fix Component 2\",\n",
    "    \"Fix Condition 3\", \"Fix Component 3\"\n",
    "]\n",
    "\n",
    "final_df = task_df[output_columns]\n",
    "final_df.to_excel(\"first_task.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
